Hyperparameters are external parameters are set 

Examples
**Learning Rate**
**Epochs**
**Batch Size**


**Learning Rate** - Determines the size of the step taken during gradient descent optimizationn.

Between 0 and 1


**Batch Size** - The number of Samples used to train at any one time
                  Could be all, one,  or some of your data (batch, stochastic or mini-batdh)
                  OFten 32, 64 and 128
                  Calculable from infrastructure



**Epochs** -  The number of times that the algorithm will process the entire training data

              Each epoch contains one or more batches
              Each epoch should see the model get closer to the desired state
              Usually a high number: 10, 100,1000 and up

Hyperparameters Parameters
• Learning rate

Determines the size of the step taken during gradient descent optimizationBetween 0 and 1

• Epochs

The number of times that the algorithm will process the entire training dataEach epoch contains one or more batchesEach epoch should see the model get closer to the desired stateUsually a high number: 10, 100, 1,000, and up


• Batch size

The number of samples used to train at any one timeCould be all, one, or some of your data (batch, stochastic, or mini-batch)Often 32, 64, and 128Calculable from infrastructure

